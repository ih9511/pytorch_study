# Optimizer 심화

딥러닝에서 **최적화(Optimization)**는 모델이 최적의 가중치를 찾아가는 과정입니다. 하지만 데이터의 특성이나 문제의 종류에 따라 **경사 하강법(Gradient Descent)**의 방식이 달라질 수 있습니다.

## 1. 확률적 경사 하강법 (SGD, Stochastic Gradient Descent)

- 개념: SGD는 배치 크기를 1개(혹은 작은 미니배치)로 설정해서 학습하는 방식입니다.
- 수식
    
    $$
    W=W-\eta\cdot\nabla L(W)
    $$
    
    - $W$ : 가중치
    - $\eta$: 학습률 (Learning Rate)
    - $\nabla L(W)$: 손실 함수 $L(W)$의 그래디언트
- 특징
    - 단순하고 메모리 효율적 → 모든 데이터셋을 저장하지 않고 즉각적으로 업데이트
    - 랜덤한 데이터 샘플을 사용 → 노이즈가 많지만 일반화 성능이 뛰어남
    - 그래디언트가 불안정하게 움직여 최적점에 수렴하기 어려움
    - 진동 문제가 발생할 수 있음
- 언제 사용하면 좋을까?
    - 대규모 데이터셋을 학습할 때 (전체 데이터셋을 한 번에 로드하기 어려울 때)
    - 빠른 학습이 필요할 때 (즉각적인 업데이트가 이루어짐)
    - 하지만 진동 문제를 해결하기 위해 모멘텀(momentum)이나 다른 기법과 함께 사용함

## 2. Momentum (SGD + Momentum)

SGD의 단점인 진동 문제를 해결하기 위해, 과거의 그래디언트 방향을 일정 부분 유지하는 방식입니다.

- 수식
    
    $$
    v_t=\beta v_{t-1}+(1-\beta)\nabla L(W)
    $$
    
    $$
    W = W-\eta v_t
    $$
    
    - $v_t$: 이전 그래디언트 방향을 유지하는 값 (velocity)
    - $\beta$: 모멘텀 계수 (일반적으로 0.9 사용)
- 특징
    - 이전 그래디언트 방향을 유지 → 진동을 줄이고 빠르게 수렴
    - 더 깊은 손실 함수에서도 효율적으로 이동
    - 여전히 학습률 튜닝이 필요
- 언제 사용하면 좋을까?
    - 경사가 급격하게 변화하는 함수(Sharp Loss Surface)를 학습할 때
    - 진동이 심한 경우에 사용하면 최적화 과정이 안정적이 됨

## 3. RMSprop (Root Mean Square Propagation)

RMSprop은 각 가중치 별로 학습률을 조절하는 방식입니다. 특히 기울기가 급격히 변하는 RNN 같은 모델에서 효과적입니다.

- 수식
    
    $$
    E[g^2]_t=\beta E[g^2]_{t-1}+(1-\beta)g^2_t
    $$
    
    $$
    W=W-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t
    $$
    
- 특징
    - 기울기가 급격히 변하는 경우 학습률을 자동 조절
    - RNN에서 기울기 폭발(Gradient Explosion) 문제를 줄여줌
    - 너무 작은 학습률이 적용될 수 있어 조절이 필요
- 왜 RNN에서 자주 사용될까?
    - RNN의 그래디언트는 시간이 지남에 따라 기울기 소멸(Gradient Vanishing) 또는 기울기 폭발(Gradient Explosion)이 발생할 가능성이 높음.
    - RMSprop은 기울기가 너무 크면 학습률을 줄이고, 너무 작으면 학습률을 증가시켜서 RNN의 기울기 문제를 완화하는 역할을 함.

## 4. Adam (Adaptive Moment Estimation)

Adam은 SGD + Momentum + RMSprop을 합친 강력한 최적화 알고리즘입니다. 그래서 현재 딥러닝에서 가장 널리 사용됩니다.

- 수식
    
    $$
    m_t=\beta_1m_{t-1}+(1-\beta_1)g_t
    $$
    
    $$
    v_t=\beta_2v_{t-1}+(1-\beta_2)g^2_t
    $$
    
    $$
    \hat{m_t}=\frac{m_t}{1-\beta^t_1}, \hat{v_t}=\frac{v_t}{1-\beta^t_2}
    $$
    
    $$
    W=W-\frac{\eta}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}
    $$
    
- 특징
    - Momentum을 적용해서 최적화 속도 증가
    - RMSprop을 적용해서 학습률을 자동 조절
    - 초기 학습률 조절이 필요 없고, 기본 설정으로도 매우 효과적
- 언제 사용하면 좋은가?
    - 대부분의 딥러닝 모델에서 기본적으로 Adam을 사용하면 무난하게 학습 가능
    - 학습률 튜닝을 하지 않아도 되는 경우에 Adam이 유리
    - 하지만, 너무 빠르게 최적점으로 수렴할 수도 있어서 SGD+Momentum보다 성능이 안 나올 수도 있음

## 5. Adagrad (Adaptive Gradient)

Adagrad는 각각의 가중치에 대해 개별적으로 학습률을 조정하는 방식입니다. 즉, 자주 업데이트되는 가중치는 학습률을 낮추고, 드물게 업데이트되는 가중치는 학습률을 높여줍니다.

- 특징
    - 희소 데이터(Sparse Data) 학습에 유리함 (예: NLP, Word Embedding)
    - 학습률 조절이 자동으로 이루어짐
- 언제 사용하면 좋을까?
    - NLP에서 Word Embedding 학습
    - 희소한 데이터를 학습할 때

### NLP, Word Embedding이 왜 희소 데이터(Sparse Data)인가?

자연어 처리에서 단어는 **엄청난 개수를 가진 범주형 데이터**라고 할 수 있습니다. 하지만 특정 단어들은 **매우 자주 등장하고, 어떤 단어들은 거의 등장하지 않습니다**.

1. 단어 분포는 희소(Sparse)하다.
    
    자연어 데이터에서는 Zipf’s Law가 적용됩니다. 즉, 소수의 단어(고빈도 단어)가 전체 문서에서 대부분의 빈도를 차지하고, 나머지 단어들은 거의 등장하지 않습니다. 예를 들어,
    
    | 단어 | 등장 횟수 |
    | ------- | ------- |
    | “the” | 100,000 |
    | “is” | 80,000 |
    | “apple” | 1,500 |
    | “quantum” | 20 |
    - “the”, “is”, “of” 같은 단어들은 거의 모든 문장에서 등장합니다.
    - 반면, “quantum”, “entropy”, “neural” 같은 단어들은 특정 문서에서만 등장합니다.
    
    이런 경우, 학습을 진행하면 **고빈도 단어는 업데이트를 자주 받고, 저빈도 단어는 거의 업데이트되지 않습니다**. 즉, NLP에서는 일부 단어 (특정 feature)가 자주 나오고, 대부분의 단어는 거의 나오지 않기 때문에 희소(Sparse)한 데이터가 됩니다.
    
2. Word Embedding에서 희소성이 더 두드러진다.
    
    딥러닝에서 Word Embedding(예: Word2Vec, FastText, GloVe 등)은 단어를 고정된 크기의 벡터로 변환하는 기법입니다. 보통 수십만 개 이상의 단어를 가지고 있으며, 각 단어는 특정 벡터 공간에서 좌표를 가집니다.
    
    예를 들어, 다음과 같은 Word Embedding이 있다고 해보죠.
    
    | 단어 | Word Embedding |
    | --- | --- |
    | “king” | [0.12, 0.55, -0.43, …] |
    | “queen” | [0.20, 0.60, -0.41, …] |
    | “neural” | [0.04, -0.02, 0.01, …] |
    | “quantum” | [0.01, -0.01, 0.00, …] |
    
    위에서 “king”, “queen” 같은 단어들은 자주 등장해서 벡터 업데이트가 자주 이루어집니다. 반면, “neural”, “quantum” 같은 단어들은 등장 횟수가 적어서 벡터가 거의 업데이트 되지 않겠죠. 즉, 희소 데이터 문제가 발생하게 됩니다. 빈도수가 낮은 단어는 학습이 느리거나 제대로 되지 않는 문제가 발생합니다.
    
3. Adagrad가 NLP에서 효과적인 이유
    
    Adagrad는 자주 등장하는 단어는 학습률을 낮추고, 드물게 등장하는 단어는 학습률을 높여서 문제를 해결합니다.
    
    $$
    W = W-\frac{\eta}{\sqrt{\sum g^2}+\epsilon}g
    $$
    
    - 기울기( $g$ )가 자주 업데이트 되는 경우 → 학습률을 자동으로 낮춤
    - 기울기가 거의 업데이트되지 않는 경우 → 학습률을 자동으로 높임